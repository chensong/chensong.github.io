---
layout: post
title: Etcd 操作
categories: [Blog, etcd]
description: Etcd 操作
keywords: etcd
---

## 1. 配置

```sh
# 使用别名
alias etcdctl="etcdctl --endpoints https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379 \
--cacert /etc/ssl/etcd/ssl/ca.pem \
--cert /etc/ssl/etcd/ssl/member-node01.pem \
--key /etc/ssl/etcd/ssl/member-node01-key.pem"
# 执行命令
export ETCDCTL_API=3 etcdctl endpoint status --write-out table
```

```sh
# 使用环境变量
export ETCDCTL_ENDPOINTS=https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379
export ETCDCTL_CACERT=/etc/ssl/etcd/ssl/ca.pem
export ETCDCTL_CERT=/etc/ssl/etcd/ssl/member-node01.pem
export ETCDCTL_KEY=/etc/ssl/etcd/ssl/member-node01-key.pem
export ETCDCTL_API=3
# 执行命令
export ETCDCTL_API=3 etcdctl endpoint status --write-out table
```

```sh
# docker 容器安装的 etcd，容器中已经配置号环境变量了
# 只显示当前 endpoint
docker exec -it etcd1 etcdctl endpoint status --write-out table
# 连接多个 endpoint
docker exec -it etcd1 sh -c "ETCDCTL_ENDPOINTS=https://192.168.1.101:2379,https://192.168.1.102:2379,https://192.168.1.103:2379 etcdctl endpoint status --write-out table"
```

## 2. 常用命令

```sh
# 列出etcd所有节点
etcdctl member list
# 删除故障节点
etcdctl member remove c13845537406e22f

# 查看状态
etcdctl endpoint status --write-out table

# 性能检测
etcdctl --write-out=table check perf

# 查看警告
etcdctl alarm list
# 执行碎片整理
etcdctl defrag
# 解除告警
etcdctl alarm disarm

# kv操作
etcdctl put foo bar
etcdctl get foo
etcdctl get foo --hex
etcdctl get --prefix foo
etcdctl get --prefix --limit=2 foo
etcdctl get --prefix --limit=10 "/registry/project.cattle.io/apprevisions/p-fstt5"
etcdctl del foo
etcdctl del --prefix foo
```

```sh
# 设置16MB的配额
etcd --quota-backend-bytes=$((16*1024*1024))
```

## 3. 故障解决

### 3.1 etcd某个节点启动报错：etcd failed to get all reachable pages

解决办法，删除节点，重新添加，步骤如下：

```sh
# 1 从集群中删除故障节点（正常节点上操作）
# 列出etcd所有节点
etcdctl member list
# 删除故障节点
etcdctl member remove c13845537406e22f

# 2 修复故障节点（故障节点上操作）
# 修改配置
sed -i  "s#initial-cluster-state: 'new'#initial-cluster-state: 'existing'"  /etc/etcd/etcd.config.yml 
# 清理节点数据（下面为默认路径，请根据你的设置修改）
rm -rf  /var/lib/etcd/member

# 3 重新添加节点（正常节点上操作）
etcdctl member add K8s-2 https://192.168.216.242:2380

# 4 重启故障节点（故障节点上操作）
systemctl restart etcd
```

### 3.2 整合压缩、碎片整理

```sh
# 1.获取当前etcd数据的修订版本(revision)
rev=$(ETCDCTL_API=3 etcdctl --endpoints=:2379 endpoint status --write-out="json" | egrep -o '"revision":[0-9]*' | egrep -o '[0-9]*')
# 2.整合压缩旧版本数据
ETCDCTL_API=3 etcdctl compact $rev
# 3.执行碎片整理
ETCDCTL_API=3 etcdctl defrag
# 4.解除告警
ETCDCTL_API=3 etcdctl alarm disarm
```

### 3.3 备份数据

```sh
# 5.备份以及查看备份数据信息
ETCDCTL_API=3 etcdctl snapshot save backup.db
ETCDCTL_API=3 etcdctl snapshot status backup.db
```

### 3.4 还原数据

#### 3.4.1

```sh
systemctl stop etcd
ETCDCTL_API=3 etcdctl snapshot restore backup.db --name etcd1 --data-dir=/var/lib/etcd
systemctl start etcd
```

```txt
恢复后的文件需要修改权限为 etcd:etcd
--name:重新指定一个数据目录，可以不指定，默认为 default.etcd
--data-dir：指定数据目录
建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir
```

#### 3.4.2

```sh
systemctl stop etcd
etcdctl snapshot restore snapshot.db --name etcd1 \
  --initial-advertise-peer-urls http://192.168.1.101:2380 \
  --initial-cluster-token k8s_etcd \
  --initial-cluster 'etcd1=http://192.168.1.101:2380,etcd2=http://192.168.1.102:2380,etcd3=http://192.168.1.103:2380' \
  --data-dir=/var/lib/etcd
systemctl start etcd
```

### 3.5 rafthttp 错误

* rafthttp: request sent was ignored
* rafthttp: request cluster ID mismatch

多节点数据不一致问题，

```sh
# 1.停止服务
systemctl stop etcd
# 2.备份所有节点数据
mv /var/lib/etcd /var/lib/etcd_bak
# 3.找到数据正确的节点
# 修改 /etc/systemd/system/etcd.service，etcd 命令添加 -force-new-cluster
-force-new-cluster
systemctl daemon-reload
systemctl start etcd
# 4.此时会使用旧数据创建一个新 etcd 集群，只包含当前节点，删除 -force-new-cluster
systemctl daemon-reload
systemctl restart etcd
# 5.此时 peer-urls 为 https://localhost:2380，添加新节点会报错，修改 --peer-urls
ETCDCTL_API=3 etcdctl member update <member-id> --peer-urls=https://192.168.1.101:2380
# 6. 这时候就可以重新添加节点了
etcdctl member add etcd2 http://192.168.1.102:2380
etcdctl member add etcd3 http://192.168.1.103:2380
```

### 3.6 服务故障恢复

在使用etcd集群的过程中，有时会出现少量主机故障，这时我们需要对集群进行维护。然而，在现实情况下，还可能遇到由于严重的设备 或网络的故障，导致超过半数的节点无法正常工作。

在etcd集群无法提供正常的服务，我们需要用到一些备份和数据恢复的手段。etcd背后的raft，保证了集群的数据的一致性与稳定性。所以我们对etcd的恢复，更多的是恢复etcd的节点服务，并还原用户数据。

首先，从剩余的正常节点中选择一个正常的成员节点， 使用 etcdctl backup 命令备份etcd数据。

```sh
$ ./etcdctl backup --data-dir /var/lib/etcd -backup-dir /tmp/etcd_backup
$ tar -zcxf backup.etcd.tar.gz /tmp/etcd_backup
```

这个命令会将节点中的用户数据全部写入到指定的备份目录中，但是节点ID,集群ID等信息将会丢失， 并在恢复到目的节点时被重新。这样主要是防止原先的节点意外重新加入新的节点集群而导致数据混乱。

然后将Etcd数据恢复到新的集群的任意一个节点上， 使用 --force-new-cluster 参数启动Etcd服务。这个参数会重置集群ID和集群的所有成员信息，其中节点的监听地址会被重置为localhost:2379, 表示集群中只有一个节点。

```sh
$ tar -zxvf backup.etcd.tar.gz -C /var/lib/etcd
$ etcd --data-dir=/var/lib/etcd --force-new-cluster ...
```

启动完成单节点的etcd,可以先对数据的完整性进行验证， 确认无误后再通过Etcd API修改节点的监听地址，让它监听节点的外部IP地址，为增加其他节点做准备。例如：

用etcd命令找到当前节点的ID。

```sh
$ etcdctl member list 

# stdout
98f0c6bf64240842: name=etcd-2 peerURLs=http://127.0.0.1:2580 clientURLs=http://127.0.0.1:2579
```

```sh
# etcdctl v3
ETCDCTL_API=3 etcdctl member update --peer-urls=https://192.168.1.101:2380

# 由于etcdctl v2不具备修改成员节点参数的功能， 下面的操作要使用API来完成。
$ curl http://127.0.0.1:2579/v2/members/98f0c6bf64240842 -XPUT \
 -H "Content-Type:application/json" -d '{"peerURLs":["http://127.0.0.1:2580"]}'
```

注意，在Etcd文档中， 建议首先将集群恢复到一个临时的目录中，从临时目录启动etcd，验证新的数据正确完整后，停止etcd，在将数据恢复到正常的目录中。

最后，在完成第一个成员节点的启动后，可以通过集群扩展的方法使用 etcdctl member add 命令添加其他成员节点进来。

## 参考文章

* <https://www.cnblogs.com/breg/p/5728237.html>
* <https://blog.51cto.com/u_14440843/3311539>
